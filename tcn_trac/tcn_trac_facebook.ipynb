{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tcn_trac_fb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gGvbDBiGhSd",
        "colab_type": "code",
        "outputId": "a63c26f3-73df-4795-ec2e-16b7e759f4cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_121nn_iGp3V",
        "colab_type": "code",
        "outputId": "12de1793-acf2-43e4-f0ea-d74c77a5cbd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Importing modules\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from keras import optimizers\n",
        "from keras.engine.topology import Layer\n",
        "from keras.models import Input, Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D,MaxPooling1D, Flatten\n",
        "from keras.layers import GlobalMaxPool1D,SpatialDropout1D,Activation, Lambda\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model,load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#Evaluation Metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TitlOB8rG9Rw",
        "colab_type": "code",
        "outputId": "86cc137b-e440-43e9-ff37-d45bb530a5b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "!pip install emoji --upgrade\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/ee/8cc0af26113508c8513dac40b1990b21d1d0136b3981a8b7b8a231a56c8d/emoji-0.5.2-py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyTFyXdYHSps",
        "colab_type": "code",
        "outputId": "1a6e2db5-9092-4637-e31d-ee19524fb1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "    trac_train = pd.read_csv('/content/drive/My Drive/trac/trac_train_dev_mstr_data.csv') #trac_train_mstr_data\n",
        "    submit = pd.read_csv('/content/drive/My Drive/trac/agr_en_fb_gold_tagged.csv') #agr_en_dev\n",
        "    submit_template = pd.read_csv('/content/drive/My Drive/trac/agr_en_fb_gold_tagged.csv', header = 0)\n",
        "    \n",
        "    trac_train.columns=[\"id\", \"comment\", \"tag\"]\n",
        "    submit.columns=[\"id\", \"comment\", \"tag\"]\n",
        "    submit_template.columns=[\"id\", \"comment\", \"pred\"]  \n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(trac_train['tag'])\n",
        "    sub_integer_encoded = label_encoder.fit_transform(submit['tag'])\n",
        "    print(integer_encoded.shape)\n",
        "\n",
        "\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    sub_integer_encoded = sub_integer_encoded.reshape(len(sub_integer_encoded), 1)\n",
        "    y_train_ohe = onehot_encoder.fit_transform(integer_encoded)\n",
        "    y_sub_ohe = onehot_encoder.fit_transform(sub_integer_encoded)\n",
        "\n",
        "    print(trac_train.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14998,)\n",
            "(14998, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr3DOGZ1HYFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  #Split input data into train and test\n",
        "  x_train, x_test, y_train, y_test = train_test_split(trac_train, y_train_ohe, test_size = 0.10, random_state = 42)\n",
        "  list_sentences_train = x_train[\"comment\"]\n",
        "  list_sentences_test = x_test[\"comment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x35rEPAiQrax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import emoji\n",
        "def demojify(text):\n",
        "  return emoji.demojize(text)\n",
        "\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    return ''.join(i for i in text if ord(i)<128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQxAuwtEHipJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Cleaning input text\n",
        "import re,string\n",
        "from string import digits\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub(' +', ' ',x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub(' +', ' ',x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.lower())\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.lower())\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.replace('user',''))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.replace('user',''))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.replace('url',''))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.replace('url',''))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('http\\S+\\s*', '', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('http\\S+\\s*', '', x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.translate(str.maketrans('','','1234567890')))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('\\n', ' ', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('\\n', ' ', x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub(' +', ' ',x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub(' +', ' ',x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.strip())\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.strip())\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(demojify)\n",
        "list_sentences_test = list_sentences_test.apply(demojify)\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.replace(':',' '))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.replace(':',' '))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.replace('_',' '))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.replace('_',' '))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.strip())\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.strip())\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub(' +', ' ',x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub(' +', ' ',x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('[%s]' % re.escape(\"\"\"!\"$%&'()*+,-./:;<=>?[\\]^_`{|}~\"\"\"), '', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('[%s]' % re.escape(\"\"\"!\"$%&'()*+,-./:;<=>?[\\]^_`{|}~\"\"\"), '', x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('@\\S+', '', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('@\\S+', '', x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(remove_non_ascii)\n",
        "list_sentences_test = list_sentences_test.apply(remove_non_ascii)\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('[^a-zA-Z0-9\\n\\.]', ' ', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('[^a-zA-Z0-9\\n\\.]', ' ', x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XESYDsrHsSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "##Main Wrapper\n",
        "\n",
        "def run_task():\n",
        "\n",
        "    max_features = 30000 \n",
        "    tokenizer = Tokenizer(num_words=max_features)\n",
        "\n",
        "    tokenizer.fit_on_texts(list(list_sentences_train))\n",
        "\n",
        "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "\n",
        "    maxlen = 300\n",
        "    X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "    X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
        "    \n",
        "    print(f'X_t.shape= {X_t.shape}')\n",
        "    print(f'X_te.shape= {X_te.shape}')\n",
        "\n",
        "\n",
        "    model = compiled_tcn(num_feat=25000,\n",
        "                                num_classes=2, \n",
        "                                nb_filters=300,\n",
        "                                kernel_size=3,\n",
        "                                embedding_param=len(tokenizer.word_index)+1, \n",
        "                                dilations=[2 ** i for i in range(4)],\n",
        "                                nb_stacks=1,\n",
        "                                max_len=300,   \n",
        "                                activation='norm_relu',\n",
        "                                use_skip_connections=True,\n",
        "                                return_sequences=True)\n",
        "\n",
        "    print(f'x_train.shape = {x_train.shape}')\n",
        "    print(f'y_train.shape = {y_train.shape}')\n",
        "    print(f'x_test.shape = {x_test.shape}')\n",
        "    print(f'y_test.shape = {y_test.shape}')\n",
        "    print(f'X_t.shape = {X_t.shape}')\n",
        "    print(f'X_te.shape = {X_te.shape}')\n",
        "    \n",
        "    model.summary()\n",
        "    #Pausing early stopping to observe convergence\n",
        "    #early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2, verbose=1)\n",
        "    #callbacks_list = [early_stopping]\n",
        "    filepath=\"/content/drive/My Drive/Deep Learning/neuron/trac/tcn_trac_weights-improvement-{epoch:02d}-{categorical_accuracy:.6f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') \n",
        "    callbacks_list = [checkpoint]    \n",
        "    hist=model.fit(X_t, y_train,batch_size=32, epochs=2,validation_data=(X_te, y_test),validation_split=0.1, shuffle=True,verbose=1) \n",
        "\n",
        "    tcn_val_model = model.predict(X_te,batch_size=32,verbose=1) \n",
        "    thresh_f1={}\n",
        "    thresh_f1_mic={}\n",
        "    thresh_f1_mac={}\n",
        "    for thresh in np.arange(0.1, 1.001, 0.01):\n",
        "        thresh = np.round(thresh, 2)\n",
        "        thresh_f1[thresh]=metrics.f1_score(y_test,(tcn_val_model>thresh).astype(int),average='weighted')\n",
        "        thresh_f1_mic[thresh]=metrics.f1_score(y_test,(tcn_val_model>thresh).astype(int),average='micro')\n",
        "        thresh_f1_mac[thresh]=metrics.f1_score(y_test,(tcn_val_model>thresh).astype(int),average='macro')\n",
        "    \n",
        "    prev_f1=0.0\n",
        "    _thresh=0.0\n",
        "    _f1=0.0\n",
        "    \n",
        "    for thresh,f1 in thresh_f1.items():\n",
        "      if f1 > prev_f1:\n",
        "        _f1=f1\n",
        "        _thresh=thresh\n",
        "        prev_f1=f1\n",
        "        \n",
        "    print(\"**F1 score: {0}\".format(_f1_mic))\n",
        "    print(\" Weighted F1 score: {0}\".format(_f1))\n",
        "    print(\" Macro F1 score: {0}\".format(_f1_mac))\n",
        "    accuracy = accuracy_score(y_test, (tcn_val_model>_thresh).astype(int))\n",
        "\n",
        "    precision = precision_score(y_test, (tcn_val_model>_thresh).astype(int),average='micro')\n",
        "    precision_mac = precision_score(y_test, (tcn_val_model>_thresh).astype(int),average='macro')\n",
        "\n",
        "    recall = recall_score(y_test, (tcn_val_model>_thresh).astype(int),average='micro')\n",
        "    recall_mac = recall_score(y_test, (tcn_val_model>_thresh).astype(int),average='macro')\n",
        "\n",
        "    print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(tcn_val_model, axis=1)))\n",
        "    roc_auc_scr = metrics.roc_auc_score(y_test, tcn_val_model, average='micro')\n",
        "    roc_auc_scr = metrics.roc_auc_score(y_test, tcn_val_model, average='macro')\n",
        "\n",
        "    prec, recal, fscore, support=precision_recall_fscore_support(np.argmax(y_test, axis=1), np.argmax(tcn_val_model, axis=1), average=None)\n",
        "\n",
        "    with open('/content/drive/My Drive/Deep Learning/thesis_submission/trac/tcn_trac_with300dnse_500f_1024emb_ks5_ml300_1stack_nf25k_bs32.csv', 'w') as file_obj:\n",
        "      file_obj.write('ID,1,2,3\\n')\n",
        "      for pred in (range(len(tcn_val_model))):#sample_submission[\"id\"]):\n",
        "        file_obj.write(str(pred) + ',' + ','.join('{:.2f}'.format(s) for s in tcn_val_model[pred].tolist()) + '\\n')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9kuy8fyHwhH",
        "colab_type": "code",
        "outputId": "03725835-1a00-4e3f-d28d-9f52ae11b55e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3509
        }
      },
      "source": [
        "\n",
        "\n",
        "def channel_normalization(x):\n",
        "    # type: (Layer) -> Layer\n",
        "    \"\"\" Normalize a layer to the maximum activation\n",
        "    This keeps a layers values between zero and one.\n",
        "    It helps with relu's unbounded activation\n",
        "    Args:\n",
        "        x: The layer to normalize\n",
        "    Returns:\n",
        "        A maximal normalized layer\n",
        "    \"\"\"\n",
        "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
        "    out = x / max_values\n",
        "    return out\n",
        "\n",
        "\n",
        "def residual_block(x, s, i, activation, nb_filters, kernel_size, dropout_rate=0):\n",
        "    # type: (Layer, int, int, str, int, int, float) -> Tuple[Layer, Layer]\n",
        "    \"\"\"Defines the residual block for the WaveNet TCN\n",
        "    Args:\n",
        "        x: The previous layer in the model\n",
        "        s: The stack index i.e. which stack in the overall TCN\n",
        "        i: The dilation power of 2 we are using for this residual block\n",
        "        activation: The name of the type of activation to use\n",
        "        nb_filters: The number of convolutional filters to use in this block\n",
        "        kernel_size: The size of the convolutional kernel\n",
        "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "    Returns:\n",
        "        A tuple where the first element is the residual model layer, and the second\n",
        "        is the skip connection.\n",
        "    \"\"\"\n",
        "    original_x = x\n",
        "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
        "                  dilation_rate=i, padding='causal',\n",
        "                  name='dilated_conv_%d_tanh_s%d' % (i, s))(x)\n",
        "    \n",
        "    if activation == 'norm_relu':\n",
        "        x = Activation('relu')(conv)\n",
        "        x = Lambda(channel_normalization)(x)\n",
        "    else:\n",
        "        x = Activation(activation)(conv)\n",
        "    x = SpatialDropout1D(dropout_rate, name='spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
        "\n",
        "    # 1x1 conv.\n",
        "    x = Conv1D(nb_filters, 1, padding='causal')(x)\n",
        "    res_x = keras.layers.add([original_x, x])\n",
        "    return res_x, x\n",
        "\n",
        "\n",
        "def process_dilations(dilations):\n",
        "    def is_power_of_two(num):\n",
        "        return num != 0 and ((num & (num - 1)) == 0)\n",
        "\n",
        "    if all([is_power_of_two(i) for i in dilations]):\n",
        "        return dilations\n",
        "\n",
        "    else:\n",
        "        new_dilations = [2 ** i for i in dilations]\n",
        "        print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n",
        "        return new_dilations\n",
        "\n",
        "\n",
        "def TCN(input_layer,\n",
        "        embedding_param,\n",
        "        nb_filters=64,\n",
        "        kernel_size=2,\n",
        "        nb_stacks=1,\n",
        "        dilations=None,\n",
        "        activation='norm_relu',\n",
        "        use_skip_connections=True,\n",
        "        dropout_rate=0.0,\n",
        "        return_sequences=True):\n",
        "    \"\"\"Creates a TCN layer.\n",
        "    Args:\n",
        "        input_layer: A tensor of shape (batch_size, timesteps, input_dim).\n",
        "        nb_filters: The number of filters to use in the convolutional layers.\n",
        "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
        "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
        "        nb_stacks : The number of stacks of residual blocks to use.\n",
        "        activation: The activations to use (norm_relu, wavenet, relu...).\n",
        "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
        "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
        "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "    Returns:\n",
        "        A TCN layer.\n",
        "    \"\"\"\n",
        "\n",
        "    ###\n",
        "    print('TCN: embedding_param=', embedding_param)\n",
        "    print('TCN: nb_filters=', nb_filters)\n",
        "    print('TCN: kernel_size=', kernel_size)\n",
        "    print('TCN: nb_stacks=', nb_stacks)\n",
        "    print('TCN: dilations=', dilations)\n",
        "    print('TCN: activation=', activation)\n",
        "    print('TCN: use_skip_connections=', use_skip_connections)\n",
        "    print('TCN: dropout_rate=', dropout_rate)\n",
        "    print('TCN: return_sequences=', return_sequences)\n",
        "    ###\n",
        "   \n",
        "    if dilations is None:\n",
        "        dilations = [1, 2, 4, 8, 16, 32]\n",
        "    embed_size = 1024\n",
        "    x = Embedding(embedding_param, embed_size)(input_layer)\n",
        "    x = Conv1D(nb_filters, 1, padding='causal', name='initial_conv')(x)  ###padding='causal'\n",
        "    skip_connections = []\n",
        "    for s in range(nb_stacks):\n",
        "        for i in dilations:\n",
        "            x, skip_out = residual_block(x, s, i, activation, nb_filters, kernel_size, dropout_rate)\n",
        "            skip_connections.append(skip_out)\n",
        "    if use_skip_connections:\n",
        "        x = keras.layers.add(skip_connections)\n",
        "    \n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if not return_sequences:\n",
        "        output_slice_index = -1\n",
        "        x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def compiled_tcn(num_feat,  # type: int\n",
        "                 num_classes,  # type: int\n",
        "                 nb_filters,  # type: int\n",
        "                 kernel_size,  # type: int\n",
        "                 embedding_param,\n",
        "                 dilations,  # type: List[int]\n",
        "                 nb_stacks,  # type: int\n",
        "                 max_len,  # type: int\n",
        "                 activation='norm_relu',  # type: str\n",
        "                 use_skip_connections=True,  # type: bool\n",
        "                 return_sequences=True,\n",
        "                 regression=False,  # type: bool\n",
        "                 dropout_rate=0.05  # type: float\n",
        "                 ):\n",
        "    # type: (...) -> keras.Model\n",
        "    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n",
        "    Args:\n",
        "        num_feat: A tensor of shape (batch_size, timesteps, input_dim).\n",
        "        num_classes: The size of the final dense layer, how many classes we are predicting.\n",
        "        nb_filters: The number of filters to use in the convolutional layers.\n",
        "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
        "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
        "        nb_stacks : The number of stacks of residual blocks to use.\n",
        "        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n",
        "        activation: The activations to use.\n",
        "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
        "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
        "        regression: Whether the output should be continuous or discrete.\n",
        "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "    Returns:\n",
        "        A compiled keras TCN.\n",
        "    \"\"\"\n",
        "\n",
        "    dilations = process_dilations(dilations)\n",
        "\n",
        "    input_layer = Input(name='input_layer', shape=(max_len, ))   ###Input(name='input_layer', shape=(max_len, num_feat))\n",
        "\n",
        "    x = TCN(input_layer,embedding_param, nb_filters, kernel_size, nb_stacks, dilations, activation,\n",
        "            use_skip_connections, dropout_rate, return_sequences)\n",
        "    print('compiled tcn: max_len=', max_len)\n",
        "    print('compiled tcn: num_feat=', num_feat)\n",
        "    print('compiled tcn: num_classes=', num_classes)\n",
        "    print('compiled tcn: embedding_param=', embedding_param)\n",
        "    print('compiled tcn: nb_filters=', nb_filters)\n",
        "    print('compiled tcn: kernel_size=', kernel_size)\n",
        "    print('compiled tcn: nb_stacks=', nb_stacks)\n",
        "    print('compiled tcn: dilations=', dilations)\n",
        "    print('compiled tcn: activation=', activation)\n",
        "    print('compiled tcn: use_skip_connections=', use_skip_connections)\n",
        "    print('compiled tcn: dropout_rate=', dropout_rate)\n",
        "    print('compiled tcn: return_sequences=', return_sequences)\n",
        "\n",
        "    if not regression:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "        x = Dense(300,activation=\"relu\")(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        ###\n",
        "        # classification\n",
        "        x = Dense(num_classes)(x)\n",
        "        x = Activation('sigmoid', name='output_sigmoid')(x)\n",
        "        output_layer = x\n",
        "        print(f'model.x = {input_layer.shape}')\n",
        "        print(f'model.y = {output_layer.shape}')\n",
        "        model = Model(input_layer, output_layer)\n",
        "\n",
        "        adam = optimizers.Adam(lr=1e-4, clipnorm=1.)\n",
        "        model.compile(adam, loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "        print('Adam with norm clipping.')\n",
        "\n",
        "    model_name = 'D-TCN_C{}_B{}_L{}'.format(2, nb_stacks, '-'.join(map(str, dilations)))\n",
        "    print(f'Model name = {model_name}.')\n",
        "    return model\n",
        "\n",
        "###Main Module\n",
        "if __name__ == '__main__':\n",
        "    run_task()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_t.shape= (13498, 500)\n",
            "X_te.shape= (1500, 500)\n",
            "X_sub.shape= (915, 500)\n",
            "TCN: embedding_param= 25209\n",
            "TCN: nb_filters= 750\n",
            "TCN: kernel_size= 4\n",
            "TCN: nb_stacks= 1\n",
            "TCN: dilations= [1, 2, 4, 8, 16, 32, 64]\n",
            "TCN: activation= norm_relu\n",
            "TCN: use_skip_connections= True\n",
            "TCN: dropout_rate= 0.05\n",
            "TCN: return_sequences= True\n",
            "compiled tcn: max_len= 500\n",
            "compiled tcn: num_feat= 25000\n",
            "compiled tcn: num_classes= 3\n",
            "compiled tcn: embedding_param= 25209\n",
            "compiled tcn: nb_filters= 750\n",
            "compiled tcn: kernel_size= 4\n",
            "compiled tcn: nb_stacks= 1\n",
            "compiled tcn: dilations= [1, 2, 4, 8, 16, 32, 64]\n",
            "compiled tcn: activation= norm_relu\n",
            "compiled tcn: use_skip_connections= True\n",
            "compiled tcn: dropout_rate= 0.05\n",
            "compiled tcn: return_sequences= True\n",
            "model.x = (?, 500)\n",
            "model.y = (?, 3)\n",
            "Adam with norm clipping.\n",
            "Model name = D-TCN_C2_B1_L1-2-4-8-16-32-64.\n",
            "x_train.shape = (13498, 3)\n",
            "y_train.shape = (13498, 3)\n",
            "x_test.shape = (1500, 3)\n",
            "y_test.shape = (1500, 3)\n",
            "X_t.shape = (13498, 500)\n",
            "X_te.shape = (1500, 500)\n",
            "X_sub.shape = (915, 500)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_layer (InputLayer)        (None, 500)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 500, 1024)    25814016    input_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_91 (Lambda)              (None, 500, 1024)    0           embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_101 (Add)                   (None, 500, 1024)    0           embedding_11[0][0]               \n",
            "                                                                 lambda_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "initial_conv (Conv1D)           (None, 500, 750)     768750      add_101[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_1_tanh_s0 (Conv1D) (None, 500, 750)     2250750     initial_conv[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 500, 750)     0           dilated_conv_1_tanh_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_92 (Lambda)              (None, 500, 750)     0           activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1_s0_0.050000 (None, 500, 750)     0           lambda_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_71 (Conv1D)              (None, 500, 750)     563250      spatial_dropout1d_1_s0_0.050000[0\n",
            "__________________________________________________________________________________________________\n",
            "add_102 (Add)                   (None, 500, 750)     0           initial_conv[0][0]               \n",
            "                                                                 conv1d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_2_tanh_s0 (Conv1D) (None, 500, 750)     2250750     add_102[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 500, 750)     0           dilated_conv_2_tanh_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_93 (Lambda)              (None, 500, 750)     0           activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_2_s0_0.050000 (None, 500, 750)     0           lambda_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_72 (Conv1D)              (None, 500, 750)     563250      spatial_dropout1d_2_s0_0.050000[0\n",
            "__________________________________________________________________________________________________\n",
            "add_103 (Add)                   (None, 500, 750)     0           add_102[0][0]                    \n",
            "                                                                 conv1d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_4_tanh_s0 (Conv1D) (None, 500, 750)     2250750     add_103[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 500, 750)     0           dilated_conv_4_tanh_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_94 (Lambda)              (None, 500, 750)     0           activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_4_s0_0.050000 (None, 500, 750)     0           lambda_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_73 (Conv1D)              (None, 500, 750)     563250      spatial_dropout1d_4_s0_0.050000[0\n",
            "__________________________________________________________________________________________________\n",
            "add_104 (Add)                   (None, 500, 750)     0           add_103[0][0]                    \n",
            "                                                                 conv1d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_8_tanh_s0 (Conv1D) (None, 500, 750)     2250750     add_104[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 500, 750)     0           dilated_conv_8_tanh_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_95 (Lambda)              (None, 500, 750)     0           activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_8_s0_0.050000 (None, 500, 750)     0           lambda_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_74 (Conv1D)              (None, 500, 750)     563250      spatial_dropout1d_8_s0_0.050000[0\n",
            "__________________________________________________________________________________________________\n",
            "add_105 (Add)                   (None, 500, 750)     0           add_104[0][0]                    \n",
            "                                                                 conv1d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_16_tanh_s0 (Conv1D (None, 500, 750)     2250750     add_105[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 500, 750)     0           dilated_conv_16_tanh_s0[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_96 (Lambda)              (None, 500, 750)     0           activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_16_s0_0.05000 (None, 500, 750)     0           lambda_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_75 (Conv1D)              (None, 500, 750)     563250      spatial_dropout1d_16_s0_0.050000[\n",
            "__________________________________________________________________________________________________\n",
            "add_106 (Add)                   (None, 500, 750)     0           add_105[0][0]                    \n",
            "                                                                 conv1d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_32_tanh_s0 (Conv1D (None, 500, 750)     2250750     add_106[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 500, 750)     0           dilated_conv_32_tanh_s0[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_97 (Lambda)              (None, 500, 750)     0           activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_32_s0_0.05000 (None, 500, 750)     0           lambda_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_76 (Conv1D)              (None, 500, 750)     563250      spatial_dropout1d_32_s0_0.050000[\n",
            "__________________________________________________________________________________________________\n",
            "add_107 (Add)                   (None, 500, 750)     0           add_106[0][0]                    \n",
            "                                                                 conv1d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_64_tanh_s0 (Conv1D (None, 500, 750)     2250750     add_107[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 500, 750)     0           dilated_conv_64_tanh_s0[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_98 (Lambda)              (None, 500, 750)     0           activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_64_s0_0.05000 (None, 500, 750)     0           lambda_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_77 (Conv1D)              (None, 500, 750)     563250      spatial_dropout1d_64_s0_0.050000[\n",
            "__________________________________________________________________________________________________\n",
            "add_109 (Add)                   (None, 500, 750)     0           conv1d_71[0][0]                  \n",
            "                                                                 conv1d_72[0][0]                  \n",
            "                                                                 conv1d_73[0][0]                  \n",
            "                                                                 conv1d_74[0][0]                  \n",
            "                                                                 conv1d_75[0][0]                  \n",
            "                                                                 conv1d_76[0][0]                  \n",
            "                                                                 conv1d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 500, 750)     0           add_109[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_99 (Lambda)              (None, 500, 750)     0           activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_110 (Add)                   (None, 500, 750)     0           activation_88[0][0]              \n",
            "                                                                 lambda_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_11 (Global (None, 750)          0           add_110[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 300)          225300      global_max_pooling1d_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 300)          0           dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 3)            903         dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "output_sigmoid (Activation)     (None, 3)            0           dense_22[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 46,506,969\n",
            "Trainable params: 46,506,969\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 13498 samples, validate on 1500 samples\n",
            "Epoch 1/3\n",
            "13498/13498 [==============================] - 304s 23ms/step - loss: 0.6303 - categorical_accuracy: 0.4113 - val_loss: 0.6133 - val_categorical_accuracy: 0.4020\n",
            "\n",
            "Epoch 00001: categorical_accuracy improved from -inf to 0.41132, saving model to /content/drive/My Drive/Deep Learning/neuron/trac/tcn_trac_weights-improvement-01-0.411320.hdf5\n",
            "Epoch 2/3\n",
            "13498/13498 [==============================] - 306s 23ms/step - loss: 0.5646 - categorical_accuracy: 0.5150 - val_loss: 0.5395 - val_categorical_accuracy: 0.5540\n",
            "\n",
            "Epoch 00002: categorical_accuracy improved from 0.41132 to 0.51504, saving model to /content/drive/My Drive/Deep Learning/neuron/trac/tcn_trac_weights-improvement-02-0.515039.hdf5\n",
            "Epoch 3/3\n",
            "13498/13498 [==============================] - 308s 23ms/step - loss: 0.4515 - categorical_accuracy: 0.6547 - val_loss: 0.5865 - val_categorical_accuracy: 0.5427\n",
            "\n",
            "Epoch 00003: categorical_accuracy improved from 0.51504 to 0.65469, saving model to /content/drive/My Drive/Deep Learning/neuron/trac/tcn_trac_weights-improvement-03-0.654690.hdf5\n",
            "915/915 [==============================] - 9s 10ms/step\n",
            " Weighted F1 score: 0.6938303132139467\n",
            "[[460 136  33]\n",
            " [ 73  62   7]\n",
            " [ 40  51  53]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}