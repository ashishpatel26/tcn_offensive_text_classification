{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxic_tcn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCWG-RkmOsfB",
        "colab_type": "code",
        "outputId": "c536fbc3-a3d5-47a9-8787-2dcbb26101ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o-ZQ6moPJGB",
        "colab_type": "code",
        "outputId": "27f5375f-c4d8-4767-bc6c-ca006ec8a0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls \"/content/drive/My Drive/Deep Learning/data/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_submission.csv  submission_gru.csv  test.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qNIYn5wtmLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing modules\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from keras import optimizers\n",
        "from keras.engine.topology import Layer\n",
        "from keras.models import Input, Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D,MaxPooling1D, Flatten\n",
        "from keras.layers import GlobalMaxPool1D,SpatialDropout1D,Activation, Lambda\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model,load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "#Evaluation Metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87F-tIEB-6bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/Deep Learning/offense/taskb/taskb_combined_mstr_data.csv',encoding='utf-8')\n",
        "submit = pd.read_csv('/content/drive/My Drive/Deep Learning/offense/taskb/testset-taskb.tsv',delimiter='\\t',encoding='utf-8')\n",
        "submit_template = pd.read_csv('/content/drive/My Drive/Deep Learning/offense/taskb/taskb_prediction_submission_template.csv', header = 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1WWMGJtUTDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#performing blank(null) check\n",
        "null_check=train.isnull().sum()\n",
        "print(null_check)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql6yIL_3_eCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split input to train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(train, train[\"tag\"], test_size = 0.10, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvh7wMxo_mH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Storing the Toxic content column for further processing\n",
        "list_sentences_train = x_train[\"comment\"]\n",
        "list_sentences_test = x_test[\"comment\"]\n",
        "list_sentences_submit = submit[\"tweet\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Afvoh8W_xc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Cleaning the text\n",
        "import re,string\n",
        "from string import digits\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    return ''.join(i for i in text if ord(i)<128)\n",
        "  \n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub(' +', ' ',x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub(' +', ' ',x))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: re.sub(' +', ' ',x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.lower())\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.lower())\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: x.lower())\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.replace('user',''))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.replace('user',''))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: x.replace('user',''))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.replace('url',''))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.replace('url',''))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: x.replace('url',''))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('http\\S+\\s*', '', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('http\\S+\\s*', '', x))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: re.sub('http\\S+\\s*', '', x))\n",
        "                                                    \n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('@\\S+', '', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('@\\S+', '', x))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: re.sub('@\\S+', '', x))  \n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', x))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', x))                                                       \n",
        "                                                    \n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(remove_non_ascii)\n",
        "list_sentences_test = list_sentences_test.apply(remove_non_ascii)\n",
        "list_sentences_submit = list_sentences_submit.apply(remove_non_ascii)\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: x.strip())\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: x.strip())\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: x.strip())\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('[^a-zA-Z0-9\\n\\.]', ' ', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('[^a-zA-Z0-9\\n\\.]', ' ', x))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: re.sub('[^a-zA-Z0-9\\n\\.]', ' ', x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub('\\n', ' ', x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub('\\n', ' ', x))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: re.sub('\\n', ' ', x))\n",
        "\n",
        "list_sentences_train = list_sentences_train.apply(lambda x: re.sub(' +', ' ',x))\n",
        "list_sentences_test = list_sentences_test.apply(lambda x: re.sub(' +', ' ',x))\n",
        "list_sentences_submit = list_sentences_submit.apply(lambda x: re.sub(' +', ' ',x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAplZZgP8TTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def channel_normalization(x):\n",
        "    # type: (Layer) -> Layer\n",
        "    \"\"\" Normalize a layer to the maximum activation\n",
        "    This keeps a layers values between zero and one.\n",
        "    It helps with relu's unbounded activation\n",
        "    Args:\n",
        "        x: The layer to normalize\n",
        "    Returns:\n",
        "        A maximal normalized layer\n",
        "    \"\"\"\n",
        "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
        "    out = x / max_values\n",
        "    return out\n",
        "\n",
        "\n",
        "def residual_block(x, s, i, activation, nb_filters, kernel_size, dropout_rate=0):\n",
        "    # type: (Layer, int, int, str, int, int, float) -> Tuple[Layer, Layer]\n",
        "    \"\"\"Defines the residual block TCN\n",
        "    Args:\n",
        "        x: The previous layer in the model\n",
        "        s: The stack index i.e. which stack in the overall TCN\n",
        "        i: The dilation power of 2 we are using for this residual block\n",
        "        activation: The name of the type of activation to use\n",
        "        nb_filters: The number of convolutional filters to use in this block\n",
        "        kernel_size: The size of the convolutional kernel\n",
        "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "    Returns:\n",
        "        A tuple where the first element is the residual model layer, and the second\n",
        "        is the skip connection.\n",
        "    \"\"\"\n",
        "    original_x = x\n",
        "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
        "                  dilation_rate=i, padding='causal',\n",
        "                  name='dilated_conv_%d_tanh_s%d' % (i, s))(x)\n",
        "    if activation == 'norm_relu':\n",
        "        x = Activation('relu')(conv)\n",
        "        x = Lambda(channel_normalization)(x)\n",
        "    else:\n",
        "        x = Activation(activation)(conv)\n",
        "\n",
        "    x = SpatialDropout1D(dropout_rate, name='spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
        "\n",
        "    # 1x1 conv.\n",
        "    x = Convolution1D(nb_filters, 1, padding='causal')(x)\n",
        "    res_x = keras.layers.add([original_x, x])\n",
        "    return res_x, x\n",
        "\n",
        "\n",
        "def process_dilations(dilations):\n",
        "    def is_power_of_two(num):\n",
        "        return num != 0 and ((num & (num - 1)) == 0)\n",
        "\n",
        "    if all([is_power_of_two(i) for i in dilations]):\n",
        "        return dilations\n",
        "\n",
        "    else:\n",
        "        new_dilations = [2 ** i for i in dilations]\n",
        "        print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n",
        "        return new_dilations\n",
        "\n",
        "\n",
        "def TCN(input_layer,\n",
        "        embedding_param,\n",
        "        nb_filters=64,\n",
        "        kernel_size=2,\n",
        "        nb_stacks=1,\n",
        "        dilations=None,\n",
        "        activation='norm_relu',\n",
        "        use_skip_connections=True,\n",
        "        dropout_rate=0.0,\n",
        "        return_sequences=True):\n",
        "    \"\"\"Creates a TCN layer.\n",
        "    Args:\n",
        "        input_layer: A tensor of shape (batch_size, timesteps, input_dim).\n",
        "        nb_filters: The number of filters to use in the convolutional layers.\n",
        "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
        "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
        "        nb_stacks : The number of stacks of residual blocks to use.\n",
        "        activation: The activations to use (norm_relu, wavenet, relu...).\n",
        "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
        "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
        "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "    Returns:\n",
        "        A TCN layer.\n",
        "    \"\"\"\n",
        "\n",
        "    ###\n",
        "    print('TCN: embedding_param=', embedding_param)\n",
        "    print('TCN: nb_filters=', nb_filters)\n",
        "    print('TCN: kernel_size=', kernel_size)\n",
        "    print('TCN: nb_stacks=', nb_stacks)\n",
        "    print('TCN: dilations=', dilations)\n",
        "    print('TCN: activation=', activation)\n",
        "    print('TCN: use_skip_connections=', use_skip_connections)\n",
        "    print('TCN: dropout_rate=', dropout_rate)\n",
        "    print('TCN: return_sequences=', return_sequences)\n",
        "    ###\n",
        "   \n",
        "    if dilations is None:\n",
        "        dilations = [1, 2, 4, 8, 16, 32]\n",
        "    embed_size = 240\n",
        "    x = Embedding(embedding_param, embed_size)(input_layer)\n",
        "\n",
        "    ###x = input_layer\n",
        "    x = Convolution1D(nb_filters, 1, padding='causal', name='initial_conv')(x) \n",
        "    skip_connections = []\n",
        "    for s in range(nb_stacks):\n",
        "        for i in dilations:\n",
        "            x, skip_out = residual_block(x, s, i, activation, nb_filters, kernel_size, dropout_rate)\n",
        "            skip_connections.append(skip_out)\n",
        "    if use_skip_connections:\n",
        "        x = keras.layers.add(skip_connections)\n",
        "    \n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if not return_sequences:\n",
        "        output_slice_index = -1\n",
        "        x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def compiled_tcn(num_feat,  # type: int\n",
        "                 num_classes,  # type: int\n",
        "                 nb_filters,  # type: int\n",
        "                 kernel_size,  # type: int\n",
        "                 embedding_param,\n",
        "                 dilations,  # type: List[int]\n",
        "                 nb_stacks,  # type: int\n",
        "                 max_len,  # type: int\n",
        "                 activation='norm_relu',  # type: str\n",
        "                 use_skip_connections=True,  # type: bool\n",
        "                 return_sequences=True,\n",
        "                 regression=False,  # type: bool\n",
        "                 dropout_rate=0.05  # type: float\n",
        "                 ):\n",
        "    # type: (...) -> keras.Model\n",
        "    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n",
        "    Args:\n",
        "        num_feat: A tensor of shape (batch_size, timesteps, input_dim).\n",
        "        num_classes: The size of the final dense layer, how many classes we are predicting.\n",
        "        nb_filters: The number of filters to use in the convolutional layers.\n",
        "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
        "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
        "        nb_stacks : The number of stacks of residual blocks to use.\n",
        "        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n",
        "        activation: The activations to use.\n",
        "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
        "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
        "        regression: Whether the output should be continuous or discrete.\n",
        "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "    Returns:\n",
        "        A compiled keras TCN.\n",
        "    \"\"\"\n",
        "    ##process tcn dilations\n",
        "    dilations = process_dilations(dilations)\n",
        "\n",
        "    input_layer = Input(name='input_layer', shape=(max_len, ))   \n",
        "\n",
        "    x = TCN(input_layer,embedding_param, nb_filters, kernel_size, nb_stacks, dilations, activation,\n",
        "            use_skip_connections, dropout_rate, return_sequences)\n",
        "\n",
        "    print('compiled tcn: x.shape=', x.shape)\n",
        "    print('compiled tcn: max_len=', max_len)\n",
        "    print('compiled tcn: num_feat=', num_feat)\n",
        "    print('compiled tcn: num_classes=', num_classes)\n",
        "    print('compiled tcn: embedding_param=', embedding_param)\n",
        "    print('compiled tcn: nb_filters=', nb_filters)\n",
        "    print('compiled tcn: kernel_size=', kernel_size)\n",
        "    print('compiled tcn: nb_stacks=', nb_stacks)\n",
        "    print('compiled tcn: dilations=', dilations)\n",
        "    print('compiled tcn: activation=', activation)\n",
        "    print('compiled tcn: use_skip_connections=', use_skip_connections)\n",
        "    print('compiled tcn: dropout_rate=', dropout_rate)\n",
        "    print('compiled tcn: return_sequences=', return_sequences)\n",
        "\n",
        "    if not regression:\n",
        "        ###Fully Connected\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "        x = Dense(50,activation=\"relu\")(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "      \n",
        "        ###\n",
        "        # classification\n",
        "        x = Dense(num_classes)(x)\n",
        "        x = Activation('sigmoid', name='output_sigmoid')(x)\n",
        "        output_layer = x\n",
        "        print(f'model.x = {input_layer.shape}')\n",
        "        print(f'model.y = {output_layer.shape}')\n",
        "        model = Model(input_layer, output_layer)\n",
        "\n",
        "        adam = optimizers.Adam(lr=1e-4, clipnorm=1.)\n",
        "        model.compile(adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        print('Adam with norm clipping.')\n",
        "\n",
        "    model_name = 'D-TCN_C{}_B{}_L{}'.format(2, nb_stacks, '-'.join(map(str, dilations)))\n",
        "    print(f'Model name = {model_name}.')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rESLal4hQTZA",
        "colab_type": "code",
        "outputId": "53323e88-2b77-44c5-f89b-5d2e07668377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4469
        }
      },
      "source": [
        "#Wrapper which generates TCN, fits the model and outputs the prediction file\n",
        "def run_task():\n",
        "\n",
        "    max_features = 20000\n",
        "    tokenizer = Tokenizer(num_words=max_features)\n",
        "\n",
        "    tokenizer.fit_on_texts(list(list_sentences_train))\n",
        "\n",
        "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "    list_tokenized_submit = tokenizer.texts_to_sequences(list_sentences_submit)\n",
        "\n",
        "    maxlen = 500\n",
        "    X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "    X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
        "    X_sub = pad_sequences(list_tokenized_submit, maxlen=maxlen)\n",
        "\n",
        "    #Compiling a TCN model with following set parameters\n",
        "    model = compiled_tcn(num_feat=20000,\n",
        "                                num_classes=6, \n",
        "                                nb_filters=500,\n",
        "                                kernel_size=3,\n",
        "                                embedding_param=len(tokenizer.word_index)+1, \n",
        "                                dilations=[2 ** i for i in range(10)],\n",
        "                                nb_stacks=1,\n",
        "                                max_len=500,    \n",
        "                                activation='norm_relu',\n",
        "                                use_skip_connections=True,\n",
        "                                return_sequences=True)\n",
        "\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "    ##Pausing Early Stopping temporarily for observing convergence \n",
        "    #early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2, verbose=1)\n",
        "    #callbacks_list = [early_stopping]\n",
        "\n",
        "    filepath=\"/content/drive/My Drive/Deep Learning/tcn_chk_points/weights-improvement-{epoch:02d}-{val_acc:.6f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint]    \n",
        "    \n",
        "    #Fitting the model\n",
        "    hist=model.fit(X_t, y_train,batch_size=32, epochs=3,validation_data=(X_te, y_test),validation_split=0.1,shuffle=True,verbose=1) ##model.fit(x_train, y_train.squeeze().argmax(axis=1), epochs=15,\n",
        "\n",
        "\n",
        "    #Prediction      \n",
        "    y_submit = model.predict(X_sub,batch_size=32,verbose=1)\n",
        "    y_submit[np.isnan(y_submit)]=0\n",
        "    sample_submission = submit_template\n",
        "    sample_submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_submit\n",
        "    sample_submission.to_csv('/content/drive/My Drive/Deep Learning/data/submission_tcn.csv', index=False)      \n",
        "\n",
        "###\n",
        "if __name__ == '__main__':\n",
        "    run_task()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TCN: embedding_param= 197608\n",
            "TCN: nb_filters= 500\n",
            "TCN: kernel_size= 3\n",
            "TCN: nb_stacks= 1\n",
            "TCN: dilations= [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
            "TCN: activation= norm_relu\n",
            "TCN: use_skip_connections= True\n",
            "TCN: dropout_rate= 0.05\n",
            "TCN: return_sequences= True\n",
            "compiled tcn: x.shape= (?, 500, 500)\n",
            "compiled tcn: max_len= 500\n",
            "compiled tcn: num_feat= 20000\n",
            "compiled tcn: num_classes= 6\n",
            "compiled tcn: embedding_param= 197608\n",
            "compiled tcn: nb_filters= 500\n",
            "compiled tcn: kernel_size= 3\n",
            "compiled tcn: nb_stacks= 1\n",
            "compiled tcn: dilations= [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
            "compiled tcn: activation= norm_relu\n",
            "compiled tcn: use_skip_connections= True\n",
            "compiled tcn: dropout_rate= 0.05\n",
            "compiled tcn: return_sequences= True\n",
            "model.x = (?, 500)\n",
            "model.y = (?, 6)\n",
            "Adam with norm clipping.\n",
            "Model name = D-TCN_C2_B1_L1-2-4-8-16-32-64-128-256-512.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_layer (InputLayer)        (None, 500)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 500, 240)     47425920    input_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "initial_conv (Conv1D)           (None, 500, 500)     120500      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_1_tanh_s0 (Conv1D) (None, 500, 500)     750500      initial_conv[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 500, 500)     0           dilated_conv_1_tanh_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_21 (Lambda)              (None, 500, 500)     0           activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1_s0_0.050000 (None, 500, 500)     0           lambda_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_1_s0_0.050000[0\n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 500, 500)     0           initial_conv[0][0]               \n",
            "                                                                 conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_2_tanh_s0 (Conv1D) (None, 500, 500)     750500      add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 500, 500)     0           dilated_conv_2_tanh_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_22 (Lambda)              (None, 500, 500)     0           activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_2_s0_0.050000 (None, 500, 500)     0           lambda_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_2_s0_0.050000[0\n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 500, 500)     0           add_23[0][0]                     \n",
            "                                                                 conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_4_tanh_s0 (Conv1D) (None, 500, 500)     750500      add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 500, 500)     0           dilated_conv_4_tanh_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_23 (Lambda)              (None, 500, 500)     0           activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_4_s0_0.050000 (None, 500, 500)     0           lambda_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_4_s0_0.050000[0\n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 500, 500)     0           add_24[0][0]                     \n",
            "                                                                 conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_8_tanh_s0 (Conv1D) (None, 500, 500)     750500      add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 500, 500)     0           dilated_conv_8_tanh_s0[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_24 (Lambda)              (None, 500, 500)     0           activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_8_s0_0.050000 (None, 500, 500)     0           lambda_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_8_s0_0.050000[0\n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 500, 500)     0           add_25[0][0]                     \n",
            "                                                                 conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_16_tanh_s0 (Conv1D (None, 500, 500)     750500      add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 500, 500)     0           dilated_conv_16_tanh_s0[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 500, 500)     0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_16_s0_0.05000 (None, 500, 500)     0           lambda_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_16_s0_0.050000[\n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 500, 500)     0           add_26[0][0]                     \n",
            "                                                                 conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_32_tanh_s0 (Conv1D (None, 500, 500)     750500      add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 500, 500)     0           dilated_conv_32_tanh_s0[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_26 (Lambda)              (None, 500, 500)     0           activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_32_s0_0.05000 (None, 500, 500)     0           lambda_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_32_s0_0.050000[\n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 500, 500)     0           add_27[0][0]                     \n",
            "                                                                 conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_64_tanh_s0 (Conv1D (None, 500, 500)     750500      add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 500, 500)     0           dilated_conv_64_tanh_s0[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_27 (Lambda)              (None, 500, 500)     0           activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_64_s0_0.05000 (None, 500, 500)     0           lambda_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_64_s0_0.050000[\n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 500, 500)     0           add_28[0][0]                     \n",
            "                                                                 conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_128_tanh_s0 (Conv1 (None, 500, 500)     750500      add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 500, 500)     0           dilated_conv_128_tanh_s0[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_28 (Lambda)              (None, 500, 500)     0           activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_128_s0_0.0500 (None, 500, 500)     0           lambda_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_128_s0_0.050000\n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 500, 500)     0           add_29[0][0]                     \n",
            "                                                                 conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_256_tanh_s0 (Conv1 (None, 500, 500)     750500      add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 500, 500)     0           dilated_conv_256_tanh_s0[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_29 (Lambda)              (None, 500, 500)     0           activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_256_s0_0.0500 (None, 500, 500)     0           lambda_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_29 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_256_s0_0.050000\n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 500, 500)     0           add_30[0][0]                     \n",
            "                                                                 conv1d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dilated_conv_512_tanh_s0 (Conv1 (None, 500, 500)     750500      add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 500, 500)     0           dilated_conv_512_tanh_s0[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_30 (Lambda)              (None, 500, 500)     0           activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_512_s0_0.0500 (None, 500, 500)     0           lambda_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, 500, 500)     250500      spatial_dropout1d_512_s0_0.050000\n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 500, 500)     0           conv1d_21[0][0]                  \n",
            "                                                                 conv1d_22[0][0]                  \n",
            "                                                                 conv1d_23[0][0]                  \n",
            "                                                                 conv1d_24[0][0]                  \n",
            "                                                                 conv1d_25[0][0]                  \n",
            "                                                                 conv1d_26[0][0]                  \n",
            "                                                                 conv1d_27[0][0]                  \n",
            "                                                                 conv1d_28[0][0]                  \n",
            "                                                                 conv1d_29[0][0]                  \n",
            "                                                                 conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 500, 500)     0           add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 500)          0           activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 50)           25050       global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 50)           0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 6)            306         dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "output_sigmoid (Activation)     (None, 6)            0           dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 57,581,776\n",
            "Trainable params: 57,581,776\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/15\n",
            "143613/143613 [==============================] - 2363s 16ms/step - loss: 0.0700 - acc: 0.9768 - val_loss: 0.0468 - val_acc: 0.9828\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.98284, saving model to /content/drive/My Drive/Deep Learning/tcn_chk_points/weights-improvement-01-0.982840.hdf5\n",
            "Epoch 2/15\n",
            "143613/143613 [==============================] - 2323s 16ms/step - loss: 0.0481 - acc: 0.9826 - val_loss: 0.0444 - val_acc: 0.9840\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.98284 to 0.98401, saving model to /content/drive/My Drive/Deep Learning/tcn_chk_points/weights-improvement-02-0.984010.hdf5\n",
            "Epoch 3/15\n",
            "143613/143613 [==============================] - 2310s 16ms/step - loss: 0.0414 - acc: 0.9844 - val_loss: 0.0452 - val_acc: 0.9830\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.98401\n",
            "Epoch 4/15\n",
            "143613/143613 [==============================] - 2310s 16ms/step - loss: 0.0358 - acc: 0.9859 - val_loss: 0.0470 - val_acc: 0.9831\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.98401\n",
            "Epoch 5/15\n",
            "143613/143613 [==============================] - 2307s 16ms/step - loss: 0.0312 - acc: 0.9877 - val_loss: 0.0493 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98401\n",
            "Epoch 6/15\n",
            "143613/143613 [==============================] - 2301s 16ms/step - loss: 0.0274 - acc: 0.9891 - val_loss: 0.0501 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.98401\n",
            "Epoch 7/15\n",
            "143613/143613 [==============================] - 2295s 16ms/step - loss: 0.0243 - acc: 0.9904 - val_loss: 0.0597 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98401\n",
            "Epoch 8/15\n",
            "143613/143613 [==============================] - 2288s 16ms/step - loss: 0.0216 - acc: 0.9914 - val_loss: 0.0602 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98401\n",
            "Epoch 9/15\n",
            "143613/143613 [==============================] - 2283s 16ms/step - loss: 0.0194 - acc: 0.9923 - val_loss: 0.0709 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98401\n",
            "Epoch 10/15\n",
            "143613/143613 [==============================] - 2279s 16ms/step - loss: 0.0171 - acc: 0.9932 - val_loss: 0.0781 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98401\n",
            "Epoch 11/15\n",
            "143613/143613 [==============================] - 2276s 16ms/step - loss: 0.0156 - acc: 0.9938 - val_loss: 0.0767 - val_acc: 0.9811\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.98401\n",
            "Epoch 12/15\n",
            "143613/143613 [==============================] - 2275s 16ms/step - loss: 0.0138 - acc: 0.9947 - val_loss: 0.0798 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.98401\n",
            "Epoch 13/15\n",
            "130816/143613 [==========================>...] - ETA: 3:15 - loss: 0.0125 - acc: 0.9952Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0Au1Ba45_08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}